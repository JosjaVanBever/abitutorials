{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the main [Index](index.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TaskManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TaskManager` is responsible for the submission of the tasks (creation of the submission script, initialization of the shell environment) as well as for the optimization of the parameters used for the parallel runs (number of MPI processes, number of OpenMP threads, automatic parallelization with ABINIT `autoparal` feature). \n",
    "\n",
    "The configuration file for the `TaskManager` is written in YAML (a good introduction to the YAML syntax can be found at http://yaml.org/spec/1.1/#id857168).\n",
    "See also http://www.yaml.org/refcard.html for the YAML reference card.\n",
    "A typical example is reported below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Resource manager e.g slurm, pbs, shell\n",
    "qtype: slurm\n",
    "# Options passed to the resource manager (syntax depends on qtype, consult the manual of your resource manager)\n",
    "qparams:\n",
    "    ntasks: 2\n",
    "    time: 0:20:00\n",
    "    partition: Oban\n",
    "# List of modules to import before running the calculation\n",
    "modules:\n",
    "    - intel/compilerpro/13.0.1.117\n",
    "    - fftw3/intel/3.3\n",
    "# Shell environment\n",
    "shell_env:\n",
    "     PATH: /home/user/local/bin/:$PATH\n",
    "     LD_LIBRARY_PATH: /home/user/local/lib:$LD_LIBRARY_PATH\n",
    "mpi_runner: /path/to/mpirun\n",
    "# Options for the automatic parallelization (Abinit autoparal feature)\n",
    "policy:\n",
    "    autoparal: 1\n",
    "    max_ncpus: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`qtype` specifies the queue resource manager (Slurm in this example). `qparams` is a dictionary with the parameters \n",
    "passed to the resource manager. \n",
    "We use the *normalized* version of the options i.e dashes in the official name of the parameter are replaced by  underscores  (for the list of supported options see ...)\n",
    "\n",
    "`modules` is the list of modules to load, while `shell_env` allows the user to specify or to modfiy the values of the environment variables.\n",
    "\n",
    "The `policy` section governs the automatic parallelization of the run: in this case abipy will use the `autoparal` features of abinit to determine an optimal configuration with **maximum** `max_ncpus` MPI nodes. Setting autoparal to 0 disables the automatic parallelization. **Other values of autoparal are not supported**.\n",
    "One can put this configuration file either in the configuration directory `$HOME/.abinit/abipy` or in the current working directory (the latter has precedence over the global configuration file located in `$HOME/.abinit/abipy`).\n",
    "\n",
    "The `TaskManager` can then be easily initialized by calling the class method `from_user_config`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Qadapter 0]\n",
      "ShellAdapter:localhost\n",
      "Hardware:\n",
      "   num_nodes: 1, sockets_per_node: 1, cores_per_socket: 2, mem_per_node 4096,\n",
      "Qadapter selected: 0\n"
     ]
    }
   ],
   "source": [
    "from abipy import abilab \n",
    "manager = abilab.TaskManager.from_user_config()\n",
    "print(manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may want to enforce some constraint on the \"optimal\" configuration. For example, you may want to select only those configurations whose parallel efficiency is greater than 0.7 and whose number of MPI nodes is divisible\n",
    "by 4. One can easily enforce this constraint via the `condition` dictionary whose syntax is similar to the one used in `mongodb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "policy:\n",
    "    autoparal: 1\n",
    "    max_ncpus: 10\n",
    "    condition: {$and: [ {\"efficiency\": {$gt: 0.7}}, {\"tot_ncpus\": {$divisible: 4}} ]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel efficiency is defined as $\\epsilon = \\dfrac{T_1}{T_N * N}$ where $N$ is the number of MPI processes and $T_j$ is the wall time \n",
    "needed to complete the calculation with $j$ MPI processes. For a perfect scaling implementation $\\epsilon$ is equal to one.\n",
    "The parallel speedup with N processors is given by $S = T_N / T_1$.\n",
    "Note that `autoparal = 1` will automatically change your `job.sh` script as well as the input file so that we can run the job in parallel with the optimal configuration required by the user. For example, you can use `paral_kgb` in GS calculations and `abipy` will automatically set the values of `npband`, `npfft`, `npkpt` ... for you! \n",
    "Note that if no configuration fulfills the given condition, abipy will use the optimal configuration that leads to the highest parallel speedup (not necessarily the most efficient one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete list of options supported by the `TaskManager` with slurm can be \n",
    "retrieved with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "# TaskManager configuration file (YAML Format)\r\n",
      "\r\n",
      "policy:\r\n",
      "    # Dictionary with options used to control the execution of the tasks.\r\n",
      "\r\n",
      "qadapters:\r\n",
      "    # List of qadapters objects (mandatory)\r\n",
      "    -  # qadapter_1\r\n",
      "    -  # qadapter_2\r\n",
      "\r\n",
      "db_connector:\r\n",
      "    # Connection to MongoDB database (optional)\r\n",
      "\r\n",
      "batch_adapter:\r\n",
      "    # Adapter used to submit flows with batch script. (optional)\r\n",
      "\r\n",
      "##########################################\r\n",
      "# Individual entries are documented below:\r\n",
      "##########################################\r\n",
      "\r\n",
      "policy: \r\n",
      "    autoparal:                # (integer). 0 to disable the autoparal feature (DEFAULT: 1 i.e. autoparal is on)\r\n",
      "    condition:                # condition used to filter the autoparal configurations (Mongodb-like syntax).\r\n",
      "                              # DEFAULT: empty i.e. ignored.\r\n",
      "    vars_condition:           # Condition used to filter the list of ABINIT variables reported by autoparal\r\n",
      "                              # (Mongodb-like syntax). DEFAULT: empty i.e. ignored.\r\n",
      "    frozen_timeout:           # A job is considered frozen and its status is set to ERROR if no change to\r\n",
      "                              # the output file has been done for `frozen_timeout` seconds. Accepts int with seconds or\r\n",
      "                              # string in slurm form i.e. days-hours:minutes:seconds. DEFAULT: 1 hour.\r\n",
      "    precedence:               # Under development.\r\n",
      "    autoparal_priorities:     # Under development.\r\n",
      "\r\n",
      "qadapter: \r\n",
      "# Dictionary with info on the hardware available on this queue.\r\n",
      "hardware:\r\n",
      "    num_nodes:           # Number of nodes available on this queue (integer, MANDATORY).\r\n",
      "    sockets_per_node:    # Number of sockets per node (integer, MANDATORY).\r\n",
      "    cores_per_socket:    # Number of cores per socket (integer, MANDATORY).\r\n",
      "                         # The total number of cores available on this queue is\r\n",
      "                         # `num_nodes * sockets_per_node * cores_per_socket`.\r\n",
      "\r\n",
      "# Dictionary with the options used to prepare the enviroment before submitting the job\r\n",
      "job:\r\n",
      "    setup:            # List of commands (strings) executed before running (DEFAULT: empty)\r\n",
      "    omp_env:          # Dictionary with OpenMP environment variables (DEFAULT: empty i.e. no OpenMP)\r\n",
      "    modules:          # List of modules to be imported before running the code (DEFAULT: empty).\r\n",
      "                      # NB: Error messages produced by module load are redirected to mods.err\r\n",
      "    shell_env:        # Dictionary with shell environment variables.\r\n",
      "    mpi_runner:       # MPI runner. Possible values in [mpirun, mpiexec, None]\r\n",
      "                      # DEFAULT: None i.e. no mpirunner is used.\r\n",
      "    shell_runner:     # Used for running small sequential jobs on the front-end. Set it to None\r\n",
      "                      # if mpirun or mpiexec are not available on the fron-end. If not\r\n",
      "                      # given, small sequential jobs are executed with `mpi_runner`.\r\n",
      "    pre_run:          # List of commands (strings) executed before the run (DEFAULT: empty)\r\n",
      "    post_run:         # List of commands (strings) executed after the run (DEFAULT: empty)\r\n",
      "\r\n",
      "# dictionary with the name of the queue and optional parameters\r\n",
      "# used to build/customize the header of the submission script.\r\n",
      "queue:\r\n",
      "    qname:            # Name of the queue (string, MANDATORY)\r\n",
      "    qparams:          # Dictionary with values used to generate the header of the job script\r\n",
      "                      # See pymatgen.io.abinit.qadapters.py for the list of supported values.\r\n",
      "\r\n",
      "# dictionary with the constraints that must be fulfilled in order to run on this queue.\r\n",
      "limits:\r\n",
      "    min_cores:         # Minimum number of cores (integer, DEFAULT: 1)\r\n",
      "    max_cores:         # Maximum number of cores (integer, MANDATORY). Hard limit to hint_cores:\r\n",
      "                       # it's the limit beyond which the scheduler will not accept the job (MANDATORY).\r\n",
      "    hint_cores:        # The limit used in the initial setup of jobs.\r\n",
      "                       # Fix_Critical method may increase this number until max_cores is reached\r\n",
      "    min_mem_per_proc:  # Minimum memory per MPI process in Mb, units can be specified e.g. 1.4 Gb\r\n",
      "                       # (DEFAULT: hardware.mem_per_core)\r\n",
      "    max_mem_per_proc:  # Maximum memory per MPI process in Mb, units can be specified e.g. `1.4Gb`\r\n",
      "                       # (DEFAULT: hardware.mem_per_node)\r\n",
      "    timelimit:         # Initial time-limit. Accepts time according to slurm-syntax i.e:\r\n",
      "                       # \"days-hours\" or \"days-hours:minutes\" or \"days-hours:minutes:seconds\" or\r\n",
      "                       # \"minutes\" or \"minutes:seconds\" or \"hours:minutes:seconds\",\r\n",
      "    timelimit_hard:    # The hard time-limit for this queue. Same format as timelimit.\r\n",
      "                       # Error handlers could try to submit jobs with increased timelimit\r\n",
      "                       # up to timelimit_hard. If not specified, timelimit_hard == timelimit\r\n",
      "    condition:         # MongoDB-like condition (DEFAULT: empty, i.e. not used)\r\n",
      "    allocation:        # String defining the policy used to select the optimal number of CPUs.\r\n",
      "                       # possible values are in [\"nodes\", \"force_nodes\", \"shared\"]\r\n",
      "                       # \"nodes\" means that we should try to allocate entire nodes if possible.\r\n",
      "                       # This is a soft limit, in the sense that the qadapter may use a configuration\r\n",
      "                       # that does not fulfill this requirement. In case of failure, it will try to use the\r\n",
      "                       # smallest number of nodes compatible with the optimal configuration.\r\n",
      "                       # Use `force_nodes` to enfore entire nodes allocation.\r\n",
      "                       # `shared` mode does not enforce any constraint (DEFAULT: shared).\r\n",
      "    max_num_launches:  # Limit to the number of times a specific task can be restarted (integer, DEFAULT: 5)\r\n",
      "\r\n",
      "\r\n",
      "qtype supported: [u'bluegene', u'moab', u'pbspro', u'sge', u'shell', u'slurm', u'torque']\r\n",
      "Use `abirun.py . manager slurm` to have the list of qparams for slurm.\r\n",
      "\r\n",
      "QPARAMS for slurm\r\n",
      "#!/bin/bash\r\n",
      "\r\n",
      "#SBATCH --partition=$${partition}\r\n",
      "#SBATCH --job-name=$${job_name}\r\n",
      "#SBATCH --nodes=$${nodes}\r\n",
      "#SBATCH --total_tasks=$${total_tasks}\r\n",
      "#SBATCH --ntasks=$${ntasks}\r\n",
      "#SBATCH --ntasks-per-node=$${ntasks_per_node}\r\n",
      "#SBATCH --cpus-per-task=$${cpus_per_task}\r\n",
      "#####SBATCH --mem=$${mem}\r\n",
      "#SBATCH --mem-per-cpu=$${mem_per_cpu}\r\n",
      "#SBATCH --hint=$${hint}\r\n",
      "#SBATCH --time=$${time}\r\n",
      "#SBATCH\t--exclude=$${exclude_nodes}\r\n",
      "#SBATCH --account=$${account}\r\n",
      "#SBATCH --mail-user=$${mail_user}\r\n",
      "#SBATCH --mail-type=$${mail_type}\r\n",
      "#SBATCH --constraint=$${constraint}\r\n",
      "#SBATCH --gres=$${gres}\r\n",
      "#SBATCH --requeue=$${requeue}\r\n",
      "#SBATCH --nodelist=$${nodelist}\r\n",
      "#SBATCH --propagate=$${propagate}\r\n",
      "#SBATCH --licenses=$${licenses}\r\n",
      "#SBATCH --output=$${_qout_path}\r\n",
      "#SBATCH --error=$${_qerr_path}\r\n",
      "$${qverbatim}\r\n"
     ]
    }
   ],
   "source": [
    "!abirun.py . doc_manager slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the main [Index](index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env3.6]",
   "language": "python",
   "name": "conda-env-env3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
