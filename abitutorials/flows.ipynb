{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the main [Index](index.ipynb) <a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks, Workflows and Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we discuss some of the basic concepts used in AbiPy \n",
    "to automate ab-initio calculations. \n",
    "In particular we will focus on the following three objects: \n",
    "\n",
    "   * `Task`\n",
    "   * `Work`\n",
    "   * `Flow`\n",
    "   \n",
    "The `Task` represent the most *elementary step* of the automatic workflow. \n",
    "Roughly speaking, it corresponds to the execution of a single Abinit input file without multiple datasets.\n",
    "\n",
    "From the point of view of AbiPy, a calculation consists of a set of `Tasks` that are connected together \n",
    "by some sort of dependency. \n",
    "Each task has a list of files that are needed to start the calculation, \n",
    "and a list of files that are produced at the end of the run.\n",
    "\n",
    "Some of the input files needed by a `Task` must be provided by the user in the form of Abinit input variables \n",
    "(e.g. the crystalline structure, the pseudopotentials), other inputs may be produced by other tasks.\n",
    "When a `Task` **B** requires the output file `DEN` of another task **A**, \n",
    "we say that **B** depends on **A** through a `DEN` file, and we express this dependency using the python dictionary:\n",
    "\n",
    "```python\n",
    "B_deps = {A: \"DEN\"}\n",
    "```\n",
    "\n",
    "To clarify this point, let's take a standard KS band structure calculation as an example.\n",
    "In this case, we have an initial `ScfTask` that solves the KS equations self-consistently to produce a `DEN` file. \n",
    "The density is then used by a second `NscfTask` to compute a band structure on an arbitrary \n",
    "set of $k$-points.\n",
    "The `NscfTask` has thus a dependency on the first `ScfTask` in the sense that it cannot be executed until the `ScfTask` is completed, and the `DEN` file produced by the `ScfTask` is threfore required.\n",
    "\n",
    "Now that we have clarified the concept of Task we can turn to the meaning of Works and Flow.\n",
    "The `Work` can be seen as a list of `Tasks`, while the `Flow` is essentially a list of `Work` objects.\n",
    "\n",
    "End-users will mainly interact with the `Flow` since this object provides an easy-to-use\n",
    "interface for performing common operations (launching jobs, checking the status of the `Tasks` etc). \n",
    "AbiPy provides several factory functions for typical first-principles calculations.\n",
    "This means that you do not need to understand all the technical details of  the python implementation.\n",
    "In many cases, indeed, we already provide some kind of `Work` or `Flow` that automates \n",
    "your calculation, and you only need to provide the correct list of input files \n",
    "that obviously must be consistent with the kind Flow/Work you are using.\n",
    "(in a nutshell you should not pass a list of inputs for performing a band structure calculation to a Work \n",
    "that is expected to compute phonons with DFPT!)\n",
    "\n",
    "All the `Works` and the `Tasks` of a flow are created and executed inside the working directory (`workdir`) \n",
    "that is usually specified by the user during the creation of the `Flow` object.\n",
    "AbiPy created the workdir of the different Works/Tasks when the `Flow` is executed\n",
    "for the first time.\n",
    "\n",
    "Each `Task` has an associated set of input variables that will be used to generate the \n",
    "Abinit input file and run the calculation. \n",
    "This input **must** be provided by the user during the creation of the `Task`.\n",
    "Fortunately, AbiPy provides an object named `AbinitInput` to facilitate the creation \n",
    "of such input. \n",
    "\n",
    "Once you have an `AbinitInput`, you can create the corresponding `Task` with the (pseudo) code:\n",
    "\n",
    "```python\n",
    "new_task = Task(abinit_input_object)\n",
    "```\n",
    "\n",
    "The `Task` provides several methods for monitoring the status of the calculation and \n",
    "for post-processing the results.\n",
    "Note that the concept of depenendency is not limited to files. All the Tasks in the \n",
    "flow are connected and can see each other. This allows programmers to implements python\n",
    "functions that will be invoked by the framework at run-time. For example, one can \n",
    "implement a Task that fetches the relaxed structure from a previous Task and \n",
    "use this configuration to start a DFPT calculation.\n",
    "\n",
    "In the next paragraph, we discuss how to construct a `Flow` for band-structure calculations\n",
    "with a high-level interface that only requires the specifications on the input files.\n",
    "This example allows us to discuss the most important methods of the `Flow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Flow  for band structure calculations\n",
    "[[back to top](#top)]\n",
    "\n",
    "Let's start by creating a function that produces two input files. \n",
    "The first input is a standard self-consistent ground-state run.\n",
    "The second input uses the density produced in the first run to perform \n",
    "a non self-consistent band structure calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # to get rid of deprecation warnings\n",
    "\n",
    "from abipy import abilab\n",
    "import abipy.flowtk as flowtk\n",
    "import abipy.data as abidata\n",
    "\n",
    "def make_scf_nscf_inputs():\n",
    "    \"\"\"Build ands return the input files for the GS-SCF and the GS-NSCF tasks.\"\"\"\n",
    "    multi = abilab.MultiDataset(structure=abidata.cif_file(\"si.cif\"),\n",
    "                              pseudos=abidata.pseudos(\"14si.pspnc\"), ndtset=2)\n",
    "\n",
    "    # Set global variables (dataset1 and dataset2)\n",
    "    multi.set_vars(ecut=6, nband=8)\n",
    "\n",
    "    # Dataset 1 (GS-SCF run)\n",
    "    multi[0].set_kmesh(ngkpt=[8,8,8], shiftk=[0,0,0])\n",
    "    multi[0].set_vars(tolvrs=1e-6)\n",
    "\n",
    "    # Dataset 2 (GS-NSCF run on a k-path)\n",
    "    kptbounds = [\n",
    "        [0.5, 0.0, 0.0], # L point\n",
    "        [0.0, 0.0, 0.0], # Gamma point\n",
    "        [0.0, 0.5, 0.5], # X point\n",
    "    ]\n",
    "\n",
    "    multi[1].set_kpath(ndivsm=6, kptbounds=kptbounds)\n",
    "    multi[1].set_vars(tolwfr=1e-12)\n",
    "    \n",
    "    # Return two input files for the GS and the NSCF run\n",
    "    scf_input, nscf_input = multi.split_datasets()\n",
    "    return scf_input, nscf_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our two input files, we pass them to the \n",
    "factory function `bandstructure_flow` that returns our `Flow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scf_input, nscf_input = make_scf_nscf_inputs()\n",
    "\n",
    "workdir = \"/tmp/hello_bands\"\n",
    "flow = flowtk.bandstructure_flow(workdir, scf_input, nscf_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bandstructure_flow` took care of creating the correct dependency between the two tasks.\n",
    "The `NscfTask`, indeed,  depends on the `ScfTask` in w0/t0, whereas the `ScfTask` has no dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: flow Pages: 1 -->\n",
       "<svg width=\"361pt\" height=\"152pt\"\n",
       " viewBox=\"0.00 0.00 360.87 152.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(12.4341 126)\">\n",
       "<title>flow</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-12.4341,26 -12.4341,-126 348.434,-126 348.434,26 -12.4341,26\"/>\n",
       "<text text-anchor=\"start\" x=\"-0.434082\" y=\"15.2\" font-family=\"Times,serif\" font-size=\"14.00\">Flow, node_id=280763, workdir=../../../../../tmp/hello_bands</text>\n",
       "<g id=\"clust1\" class=\"cluster\"><title>clusterw0</title>\n",
       "<path fill=\"#e0eeee\" stroke=\"black\" d=\"M101.238,0.359112C101.238,0.359112 235.238,0.359112 235.238,0.359112 241.238,0.359112 247.238,-5.64089 247.238,-11.6409 247.238,-11.6409 247.238,-109.641 247.238,-109.641 247.238,-115.641 241.238,-121.641 235.238,-121.641 235.238,-121.641 101.238,-121.641 101.238,-121.641 95.2381,-121.641 89.2381,-115.641 89.2381,-109.641 89.2381,-109.641 89.2381,-11.6409 89.2381,-11.6409 89.2381,-5.64089 95.2381,0.359112 101.238,0.359112\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.238\" y=\"-106.441\" font-family=\"Times,serif\" font-size=\"14.00\">BandStructureWork (w0)</text>\n",
       "</g>\n",
       "<!-- w0_t0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>w0_t0</title>\n",
       "<ellipse fill=\"#ff0000\" stroke=\"#ff0000\" cx=\"198.476\" cy=\"-73.2818\" rx=\"27.8878\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"198.476\" y=\"-74.8818\" font-family=\"Times,serif\" font-size=\"8.00\">w0_t0</text>\n",
       "<text text-anchor=\"middle\" x=\"198.476\" y=\"-66.8818\" font-family=\"Times,serif\" font-size=\"8.00\">ScfTask</text>\n",
       "</g>\n",
       "<!-- w0_t1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>w0_t1</title>\n",
       "<ellipse fill=\"#c85064\" stroke=\"#c85064\" cx=\"141.476\" cy=\"-25.2818\" rx=\"30.8467\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.476\" y=\"-26.8818\" font-family=\"Times,serif\" font-size=\"8.00\">w0_t1</text>\n",
       "<text text-anchor=\"middle\" x=\"141.476\" y=\"-18.8818\" font-family=\"Times,serif\" font-size=\"8.00\">NscfTask</text>\n",
       "</g>\n",
       "<!-- w0_t0&#45;&gt;w0_t1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>w0_t0&#45;&gt;w0_t1</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M181.374,-58.8802C176.921,-55.1298 172.023,-51.005 167.229,-46.9683\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"169.261,-44.1038 159.357,-40.3396 164.752,-49.4582 169.261,-44.1038\"/>\n",
       "<text text-anchor=\"middle\" x=\"188.688\" y=\"-41.7242\" font-family=\"Times,serif\" font-size=\"14.00\">DEN</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x10d4fe390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.get_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Note that we haven't used `getden2 = -1` in the second dataset \n",
    "since AbiPy knows how to connect the two Tasks.\n",
    "So no neeed for `get*` or `ird*` variables with Abipy. \n",
    "Just specify the correct dependency and python will do the rest!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have useful information on the status of the flow, one uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Work #0: <BandStructureWork, node_id=280764, workdir=../../../../../tmp/hello_bands/w0>, Finalized=False\n",
      "+--------+-------------+---------+--------------+------------+----------+-----------------+--------+-----------+\n",
      "| Task   | Status      | Queue   | MPI|Omp|Gb   | Warn|Com   | Class    | Sub|Rest|Corr   | Time   |   Node_ID |\n",
      "+========+=============+=========+==============+============+==========+=================+========+===========+\n",
      "| w0_t0  | Initialized\u001b[0m | None    | 1|  1|2.0    | 3|  0      | ScfTask  | (0, 0, 0)       | None   |    280765 |\n",
      "+--------+-------------+---------+--------------+------------+----------+-----------------+--------+-----------+\n",
      "| w0_t1  | Initialized\u001b[0m | None    | 1|  1|2.0    | 4|  0      | NscfTask | (0, 0, 0)       | None   |    280766 |\n",
      "+--------+-------------+---------+--------------+------------+----------+-----------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flow.show_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning of the different columns:\n",
    "    \n",
    "   * *Task*: short name of the task (usually *w[index_of_work_in_flow]_t[index_of_task_in_work]*\n",
    "   * *Status*: Status of the task\n",
    "   * *Queue*: QueueName@Job identifier returned by the resource manager when the task is submitted\n",
    "   * *(MPI|Omp|Gb)*: Number of MPI procs, OMP threads, and memory per MPI proc\n",
    "   * *(Warn|Com)*: Number of Error/Warning/Comment messages found in the ABINIT log\n",
    "   * *Class*: The class of the `Task`\n",
    "   * *(Sub|Rest|Corr)*: Number of (submissions/restart/AbiPy corrections) performed\n",
    "   * *Node_ID* : identifier of the task, used to select tasks or works in python code or `abirun.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `Flow` and `Work` are iterable. \n",
    "Iterating on a `Flow` gives `Work` objects, whereas\n",
    "iterating over a `Work` gives the `Tasks` inside that particular `Work`.\n",
    "\n",
    "```python\n",
    "for work in flow:\n",
    "    for task in work:\n",
    "        print(task)\n",
    "```\n",
    "\n",
    "`Flows` and `Works` are containers and we can select items in these containers\n",
    "with the syntax: flow[start:stop] or work[start:stop].\n",
    "This means that the previous loop is equivalent to the much more verbose version: \n",
    "\n",
    "```python\n",
    "for i in range(len(flow)):\n",
    "    work = flow[i]\n",
    "    for t in range(len(work):\n",
    "        print(work[t])\n",
    "```\n",
    "\n",
    "At this point it should not be so difficult to understand that:\n",
    "\n",
    "```python\n",
    "flow[0][0]\n",
    "```\n",
    "\n",
    "gives the first task in the first work of the flow while\n",
    "\n",
    "```python\n",
    "flow[-1][-1]\n",
    "```\n",
    "\n",
    "selects the last Task in the last Work.\n",
    "In several cases, we only need to iterate over a flat list of tasks without caring about the works.\n",
    "In this case, we can use:\n",
    "\n",
    "```python\n",
    "for task in flow.iflat_tasks():\n",
    "    print(task)\n",
    "```\n",
    "\n",
    "to iterate over all Tasks in the Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build and run the Flow\n",
    "[[back to top](#top)]\n",
    "\n",
    "The flow is still in memory and no file has been produced. In order to build the workflow, use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nFound node_id 280639 in file:\n\n  /tmp/hello_bands/.nodeid\n\nwhile the node_id of the present flow is 280763.\nThis means that you are trying to build a new flow in a directory already used by another flow.\nPossible solutions:\n   1) Change the workdir of the new flow.\n   2) remove the old directory either with `rm -r` or by calling the method flow.rmtree()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-38a28c2f54f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!rm -rf /tmp/hello_bands/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_and_pickle_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git_repos/pymatgen/pymatgen/io/abinit/flows.py\u001b[0m in \u001b[0;36mbuild_and_pickle_dump\u001b[0;34m(self, abivalidate)\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0mthe\u001b[0m \u001b[0mabinit\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mraise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \"\"\"\n\u001b[0;32m-> 1641\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabivalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/pymatgen/pymatgen/io/abinit/flows.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m                        \u001b[0;34m\"   2) remove the old directory either with `rm -r` or by calling the method flow.rmtree()\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m                         % (node_id, nodeid_path, self.node_id))\n\u001b[0;32m-> 1620\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nFound node_id 280639 in file:\n\n  /tmp/hello_bands/.nodeid\n\nwhile the node_id of the present flow is 280763.\nThis means that you are trying to build a new flow in a directory already used by another flow.\nPossible solutions:\n   1) Change the workdir of the new flow.\n   2) remove the old directory either with `rm -r` or by calling the method flow.rmtree()\n"
     ]
    }
   ],
   "source": [
    "#!rm -rf /tmp/hello_bands/\n",
    "flow.build_and_pickle_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates the directories of the `Flow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree /tmp/hello_bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'a have a look at the files/directories associated to the first work (flow[0]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow[0].get_graphviz_dirtree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`w0` is the directory containing the input files of the first workflow (well, we have only one workflow in our example).\n",
    "`t0` and `t1` contain the input files need to run the SCF and the NSC run, respectively.\n",
    "\n",
    "You might have noticed that each `Task` directory present the same structure:\n",
    "    \n",
    "   * *run.abi*: Input file\n",
    "   * *run.files*: Files file\n",
    "   * *job.sh*: Submission script\n",
    "   * *outdata*: Directory containing output data files\n",
    "   * *indata*: Directory containing input data files \n",
    "   * *tmpdata*: Directory with temporary files\n",
    "   \n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "`__AbinitFlow__.pickle` is the pickle file used to save the status of `Flow`. **Don't touch it!** \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Abinit Task *has* an [AbinitInput](abinit_input.ipynb) that in turns has a [Structure](structure.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " flow[0][0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow[0][0].input.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in flow[0][0].input.pseudos: \n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the value of `kptopt` for all tasks in our flow with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([task.input[\"kptopt\"] for task in flow.iflat_tasks()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that, in this particual case, gives the same result as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([task.input[\"kptopt\"] for task in flow[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing a Flow\n",
    "[[back to top](#top)]\n",
    "\n",
    "The `Flow` can be executed with two different approaches: a programmatic interface based \n",
    "on `flow.make_scheduler` or the `abirun.py` script. \n",
    "In this section, we discuss the first approach because it plays well with the jupyter notebook.\n",
    "Note however that `abirun.py` is the recommended approach when running non-trivial calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.make_scheduler().start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow keep track of the different actions performed by the python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the logs carefull, you will realize that in the first iteration of the scheduler, \n",
    "only the `ScfTask` is executed because the second task depends on it. \n",
    "After the initial submission, the scheduler starts to monitor all the tasks in the flow.\n",
    "\n",
    "When the ScfTask completes, the dependency of the NscfStack is fullfilled and \n",
    "a new submission takes place. Once the second task completes, the scheduler calls `flow.finalize`\n",
    "to execute (optional) logic that is supposed to be executed to perform some sort of cleanup or final processing.\n",
    "At this point, all the tasks in the flow are completed and the scheduler exits.\n",
    "\n",
    "Now we can have a look at the different output files produced by our flow with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow[0].get_graphviz_dirtree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or list only the files with a given extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.listext(\"GSR.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about the flow is that the object knows how to locate and interpret the \n",
    "different input/ouput files produced by Abinit. \n",
    "As a consequence, it's very easy to expose the AbiPy post-processing tools with a easy-to-use API\n",
    "in which only tasks/works/flow plus a very few input arguments are involved.\n",
    "\n",
    "Let's call, for instance, the inspect method to plot the self-consisten cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.inspect(tight_layout=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the other AbiPy tutorias, we have explaned how to use abiopen to create \n",
    "python objects from netcdf files. \n",
    "Well, the same code can be reused with the flow. \n",
    "It's just a matter of replacing\n",
    "\n",
    "```python\n",
    "with abiopen(filepath) as gsr:\n",
    "```\n",
    "\n",
    "with\n",
    "\n",
    "```python\n",
    "with task.open_gsr() as gsr:\n",
    "```\n",
    "\n",
    "Note that there's no need to specify the file path when you use the task-based API, because \n",
    "the `Task` knows how to locate its `GSR.nc` output.\n",
    "Let's do some practice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flow[0][0].open_gsr() as gsr:\n",
    "    ebands_kmesh = gsr.ebands\n",
    "    \n",
    "with flow[0][1].open_gsr() as gsr:\n",
    "    gsr.ebands.plot_with_edos(ebands_kmesh.get_edos());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Works, Tasks and dependencies \n",
    "[[back to top](#top)]\n",
    "\n",
    "In the previous example, we have costructed a workflow for band structure calculations\n",
    "starting from two input files and the magic line\n",
    "\n",
    "```python\n",
    "flow = flowtk.bandstructure_flow(workdir, scf_input, nscf_input)\n",
    "```\n",
    "\n",
    "Now it's the right time to explain in more details the syntax and the API used in AbiPy \n",
    "to build a flow with dependencies.\n",
    "Let's try to build a `Flow` from scratch and use graphviz after each step to show what's happening.\n",
    "We start with an empty flow in the `hello_flow` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_flow = flowtk.Flow(workdir=\"hello_flow\")\n",
    "hello_flow.get_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a new Task by just passing an `AbinitInput` for SCF calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_flow.register_scf_task(scf_input, append=True)\n",
    "hello_flow.get_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tricky part. \n",
    "We want to register a NSCF calculation that should depend on the `scf_task` in `w0_t0` via the DEN file.\n",
    "We can use the same API but we **must** specify the dependency between the two steps with the \n",
    "```{scf_task: \"DEN\"}``` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_flow.register_nscf_task(nscf_input, deps={hello_flow[0][0]: \"DEN\"}, append=True)\n",
    "hello_flow.get_graphviz(engine=\"dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we managed to build our first AbiPy flow with inter-dependent tasks in just six lines \n",
    "of code (including the three calls to graphviz).\n",
    "Now let's assume we want to add a second Nscf calculation (`NscTask`) in which we change one of the input parameters\n",
    "e.g. the number of bands and that, for some reason, we really want to re-use the output WFK file \n",
    "produced by `w0_t1` to initialize the eigenvalue solver (obviously we still need a DEN file).\n",
    "How can we express this with AbiPy? \n",
    "\n",
    "Well, the syntax for the new deps, it's just:\n",
    "\n",
    "```python\n",
    "deps = {hello_flow[0][0]: \"DEN\", hello_flow[0][1]: \"WFK\"}\n",
    "```\n",
    "\n",
    "but we should also change the input variable nband in the `nscf_input` before creating\n",
    "the new `NscTask` (remember that building a `Task` requires an `AbinitInput` object \n",
    "and a list of dependencies, if any).\n",
    "\n",
    "Now there are two ways to increase nband: the **wrong** way and the **correct** one!\n",
    "Let's start from the *wrong* way because it's always useful to learn from our mistakes.\n",
    "Let's print some values just for the record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = flow[0][1]\n",
    "print(\"nband in the first NscfTask:\", t1.input[\"nband\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the \"recipe\" recommended to us by the FORTRAN guru of our group: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just copy the previous input and change nband, it's super-easy, the FORTRAN guru said!\n",
    "new_input = t1.input\n",
    "new_input[\"nband\"] = 1000\n",
    "\n",
    "print(\"nband in the first NscfTask:\", t1.input[\"nband\"])\n",
    "print(\"nband in the new input:\", new_input[\"nband\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada! Thanks to the trick of our beloved FORTRAN guru, we ended up with *two* NscfTaks\n",
    "with the same number of bands (1000!). Why?\n",
    "\n",
    "Because `AbinitInput` is implemented internally with a dictionary, python dictionaries are mutable\n",
    "and python variables are essentially pointers (they do not store data, actually they store the address of the data).\n",
    "For a more techical explanation read here.\n",
    "\n",
    "To avoid this mistake we need to *copy* the object before changing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.input[\"nband\"] = 8 # back to the old value\n",
    "\n",
    "new_input = t1.input.new_with_vars(nband=1000)  # Copy and change nband\n",
    "\n",
    "print(\"nband in the first NscfTask:\", t1.input[\"nband\"])\n",
    "print(\"nband in the new input:\", new_input[\"nband\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally add the second `NscfTask` with 1000 bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_flow.register_nscf_task(new_input, deps={hello_flow[0][0]: \"DEN\", hello_flow[0][1]: \"WFK\"}, append=True)\n",
    "hello_flow.get_graphviz(engine=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([task.input[\"nband\"] for task in hello_flow.iflat_tasks()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that AbiPy dependencies can also be fulfilled with external files that are already available\n",
    "when the flow is constructed. There's no change in the syntax we've used so far.\n",
    "It's just a matter of using the absolute path to the DEN file as keyword of the dictionary instead of a `Task`.\n",
    "Let's start with a new `Flow` to avoid confusion and create a `NscfTask` that will start from a pre-computed `DEN` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_with_file = flowtk.Flow(workdir=\"flow_with_file\")\n",
    "\n",
    "den_filepath = abidata.ref_file(\"si_DEN.nc\")\n",
    "flow_with_file.register_nscf_task(nscf_input, deps={den_filepath: \"DEN\"})\n",
    "\n",
    "flow_with_file.get_graphviz(engine=\"dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to `new_with_vars` inside a python `for` loop is all we need to add other two `NscfTasks`\n",
    "with different `nband`, all starting from the same DEN file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nband in [10, 20]:\n",
    "    flow_with_file.register_nscf_task(nscf_input.new_with_vars(nband=nband), \n",
    "                                      deps={den_filepath: \"DEN\"}, append=False)\n",
    "\n",
    "print([task.input[\"nband\"] for task in flow_with_file.iflat_tasks()])\n",
    "flow_with_file.get_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you may ask why we need `Works` since all the examples presented so far \n",
    "mainly involve the `Flow` object.\n",
    "\n",
    "The answer is that `Works` allow us to encapsulate reusable logic in magic boxes \n",
    "that can perform lot of useful work. \n",
    "These boxes can then be connected together to generate more complicated workflows.\n",
    "We have already encountered the `BandStructureWork` at the beginning of this lesson\n",
    "and now it's time to introduce another fancy animal of the AbiPy zoo, the `PhononWork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flowtk.PhononWork.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flowtk.PhononWork.from_scf_task.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docstring seems to suggest that if I have a `scf_task`, I can construct a magic box\n",
    "to compute phonons but wait, I already have such a task! \n",
    "Actually I already have another magic box to compute the electronic band structure \n",
    "and it would be really great if I could compute the electronic and vibrational properties in a single flow.\n",
    "Let's connect the two boxes together with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new flow.\n",
    "ph_flow = flowtk.Flow(workdir=\"phflow\")\n",
    "\n",
    "# Band structure (SCF + NSCF)\n",
    "bands_work = flowtk.BandStructureWork(scf_input, nscf_input, dos_inputs=None)\n",
    "ph_flow.register_work(bands_work)    \n",
    "    \n",
    "# Build second work from scf_task.\n",
    "scf_task = bands_work[0]\n",
    "ph_work = flowtk.PhononWork.from_scf_task(scf_task, [2, 2, 2], is_ngqpt=True, tolerance=None)\n",
    "ph_flow.register_work(ph_work) \n",
    "\n",
    "ph_flow.get_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it turns out that the `PhononWork` merges all the DDB files produced by its `PhononTask`\n",
    "and put this final output file in its outdir. \n",
    "So from the AbiPy perspective, a `PhononWork` is not that different from a `ScfTask` that produces e.g. a DEN file.\n",
    "This means that we can connect other magic boxes to out `PhononWork` e.g. a set of `EPhTasks` that \n",
    "require a DDB file and another input file with the DFPT potentials \n",
    "(DVDB, merged by `PhononWork` similarly to what is done for the DDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPH tasks require 3 input files (WFK, DDB, DVDB)\n",
    "eph_deps = {ph_flow[0][1]: \"WFK\", ph_work: [\"DDB\", \"DVDB\"]}\n",
    "\n",
    "for i, ecut in enumerate([2, 3, 4]):\n",
    "    ph_flow.register_eph_task(nscf_input.new_with_vars(ecut=ecut), deps=eph_deps, append=(i != 0))\n",
    "                           \n",
    "ph_flow.get_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explains why in AbiPy we have this classification in terms of `Tasks/Works/Flows`.\n",
    "As a consequence, we can implement highly specialized `Works/Tasks` to\n",
    "solve specific problems and then connect all these nodes together.\n",
    "Just 11 lines of code to get electrons + phonons + (electrons + phononons)!\n",
    "\n",
    "But wait, did you see the gorilla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"vJG698U2Mvo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's indeed a bug in the last step. The connections among the nodes are OK but we made\n",
    "a mistake while creating the EPhTasks with:\n",
    "\n",
    "```python\n",
    "ph_flow.register_eph_task(nscf_input.new_with_vars(ecut=ecut), ...)\n",
    "```\n",
    "\n",
    "because we passed an input for a standard band structure calculation to something that is supposed\n",
    "to deal with E-PH interaction.\n",
    "This essentially to stress that the AbiPy `Flow`, by design, does not try to validate your input to make sure\n",
    "it is consistent with the workflow logic.\n",
    "This is done on purpose for two reasons:\n",
    "\n",
    "- Expert users should be able to customize/tune their input files and validating all the possible cases in python is not trivial\n",
    "- Only Abinit knows at run-time if your input file makes sense and we can't reimplement the same logic in python\n",
    "\n",
    "At this pont, you may wonder why we have so many different Abipy Tasks (`ScfTask`, `NscfTask`, `RelaxTask`, `PhononTask`, `EPHTask` ...) if there's no input validation when we create them...\n",
    "\n",
    "The answer is that we need all these subclasses to implement extra logic that is specific to that particular calculation. Abipy, indeed, is not just submitting jobs. It also monitors the evolution of the calculation\n",
    "and execute pre-defined code to fix run-time problems (and these problems are calculation specific).\n",
    "An example will help clarify this point.\n",
    "\n",
    "Restarting jobs is one of the typical problem encountered in ab-initio calculations\n",
    "and restarting a `RelaxTask` requires a different logic from e.g. restarting a `ScfTask`.\n",
    "In the case of a `ScfTask` we only need to use the output WFK (DEN) of the previous execution \n",
    "as input of the restarted job while a `RelaxTask` must also re-use the (unconverged) final structure \n",
    "of the previous job to be effective and avoid a possibly infinite loop.\n",
    "In a nutshell, when you are using a particular `Task/Work` class you are telling AbiPy how to handle possible\n",
    "problems at run-time and you are also specifying the actions that should be performed \n",
    "at the beginning/end of the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abirun.py\n",
    "[[back to top](#top)]\n",
    "\n",
    "Executing `flow.make_scheduler().start()` inside a jupyter notebook is handy if you are dealing with small calculations that require few seconds or minutes. \n",
    "This approach, however, is unpractical when you have large flows or big calculations requiring hours or days, \n",
    "even on massively parallel machines.\n",
    "In this case, indeed, one would like to run the scheduler in a separate process in the backgroud \n",
    "so that the scheduler is not killed when the jupyter server is closed.\n",
    "\n",
    "To start the scheduler in a separate process, use the `abirun.py` script.\n",
    "The syntax is:\n",
    "\n",
    "    abirun.py flow_workdir COMMAND\n",
    "\n",
    "where `flow_workdir` is the directory containing the `Flow` \n",
    "(the directory with the pickle file) and `command` selects the operation to be performed.\n",
    "\n",
    "Typical examples:\n",
    "\n",
    "    abirun.py /tmp/hello_bands status\n",
    "    \n",
    "checks the status of the `Flow` and print the results to screen while\n",
    "\n",
    "    nohup abirun.py /tmp/hello_bands scheduler > sched.log 2> sched.err &\n",
    "    \n",
    "starts the scheduler in the background redirecting the standard output to file `sched.log`\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "`nohup` is a standard Unix tool. The command make the scheduler immune \n",
    "to hangups so that you can close the shell session without killing the scheduler.\n",
    "</div>\n",
    "\n",
    "This brings us to the last and most crucial question. \n",
    "How do we configure AbiPy to run Abinit workflows on different architectures ranging from \n",
    "standard laptopts to high-performance supercomputers?\n",
    "\n",
    "Unfortunately this notebook is already quite long and these details are best covered \n",
    "in a technical documentation.\n",
    "What should be stressed here is that the behaviour can be customized with two Yaml files.\n",
    "All the information related to your environment (Abinit buid, modules, resource managers, shell environment)\n",
    "are read from the `manager.yml` configuration file, that is usually located in the directory `~/.abinit/abipy/`\n",
    "The options for the python scheduler responsible for job submission are given in `scheduler.yml`.\n",
    "\n",
    "For a more complete description of these configuration options, \n",
    "please consult the [TaskManager documentation](http://abinit.github.io/abipy/workflows/taskmanager.html).\n",
    "A list of configuration files for different machines and clusters is available \n",
    "[here](http://abinit.github.io/abipy/workflows/manager_examples.html)\n",
    "while the [Flows HOWTO](http://abinit.github.io/abipy/flows_howto.html)\n",
    "gathers answers to frequently asked questions.\n",
    "\n",
    "Last but not least, check out our \n",
    "[gallery of AbiPy Flows](http://abinit.github.io/abipy/flow_gallery/index.html) for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the main [Index](index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env3.6]",
   "language": "python",
   "name": "conda-env-env3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
